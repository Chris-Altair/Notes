ps：没想到我工作中干了不少数据抽取的事（可惜代码我没存下来），所以稍微整理下，注意以下所说的”数据抽取“指的都是通过代码实现而不是指的通过Kettle等工具实现，适用于小而灵活的、需要自己实现的场景，大场面老实用业界成熟方案吧。此外下面也纯属个人理解的，不一定正确，仅供参考。

## 1. 一些概念准备

- 数据抽取：简单理解，将数据从数据源source提取，并通过某种规则rule转换成另一种格式，然后存到目标系统target中

- source：包括但不限于数据库，可能是excel、txt甚至可能是自己定义的二进制文件等。

- target：跟source一样，一般是数据库，但也不排除其他。

- extraction rule：抽取规则，决定抽取哪些数据。

- conversion rule：转换规则，将source数据的格式转成target的格式

- storage rule：数据存储策略，决定如何将数据存储。

- exception handle：对于处理过程出现失败的数据的处理方式。

  基本上面这些概念已经能满足一个完备的数据抽取实现了。

  一个最简单也最典型的数据抽取：从数据库A中a表的数据转换一下存到B库的b表中。

## 2. 基本思路

**关键词：分批次、多线程、分步骤、异常处理**

首先一个最基本的思路是按**批次**处理，毕竟对数据量较大的情况下，一次性全把数据取出来对服务器不是很友好。所以统一的处理单位是批次，抽取按批次、转换按批次、存储也按批次。

ps：批次的处理实现是使用**垂直**（每个批次链式调用）还是**水平**（传统的过程式处理）是一个值得思考的问题。

充分利用资源可以考虑**多线程**，不同线程池处理不同种类的任务。主要是利用数据抽取是io密集型任务，利用io空闲时间可以做转换等其他不需要io的操作。

**异常处理**也很关键，如果某个批次处理出现错误了，不应该影响其他批次，这也是使用多线程的原因；此外若某个批次出现错误，则需要记录这批出错的数据，以便人工排查，例如可以考虑记录到错误表中，或者写到某个文件中。

此外还可以根据数据转换比例（例如源：目标=5：1之类的）考虑合并批次等”高级“操作，但这并非必须，还是得结合实际情况看是否使用。

程序步骤大体如下（根据实现方式，代码结构可能有很大不同，但逻辑是相似的）：

```
按批次抽取数据，将批次提交到【转换任务队列】
转换线程池监听【转换任务队列】处理批次，处理成功将批次提交到【存储任务队列】
存储线程池监听【存储任务队列】处理批次，成功执行完说明这一批次执行成功
中途任一步骤处理失败则将批次提交到【异常处理队列】，由异常处理线程池处理
```

## 3. 抽取规则

抽取规则主要考虑两个方面的问题：

1. 业务上：按哪些条件抽数据，例如按时间范围、按状态等等
2. 性能上：根据source的不同，实现抽取的方式也不同

下面列一些常用的source种类：

1. 数据库：最常见的方式，大量抽取的时候要考虑**根据索引按批次抽取**，如果是多数据源的话，可以考虑维护个数据库连接池map，每个线程处理前维护个threadLocal存要用的连接池，处理完再清除掉。
2. excel：按行范围生成批次。
3. 文本文件：json、xml、txt之类。
4. 二进制文件：根据文件结构读文件转成对象。

## 4. 转换规则

可以考虑使用注解、反射等方式将源数据类转成目标类

## 5. 存储规则

一般使用多线程存，当然如果target是单一文件之类的情况肯定不考虑这种情况了。
